# ğŸ¯ Grammar Scoring Engine â€” Technical Overview

### ğŸ§© Problem Statement

The objective of this project is to build an automated **Grammar Scoring Engine** that processes spoken audio responses and assigns a **Grammar MOS (Mean Opinion Score)** between **1.0 and 5.0**, at 0.5-point granularity. The model must learn to infer grammar quality â€” a deeply **subjective and context-sensitive linguistic construct** â€” directly from **raw waveform audio** using machine learning.

---

### ğŸ› ï¸ Core Challenges

This task presented multiple technical and real-world constraints:

| Challenge                        | Description                                                                 |
|----------------------------------|-----------------------------------------------------------------------------|
| **Data Regime**                  | **444 training audio files**, requiring aggressive transfer learning   |
| **Continuous Output Prediction** | Requires fine-grained **regression**, not classification                   |
| **Acoustic Variability**         | Varying accents, intonation, recording conditions, and pacing              |
| **Latency Constraints**          | Needs to be efficient enough for near-real-time evaluation pipelines       |

---

### ğŸš§ Our Dual-Track Strategy

To address the above challenges **robustly and modularly**, we designed **two complementary pipelines**, each targeting a different balance between computational complexity and expressive power:

| Module                | Strategy                                                                 |
|-----------------------|--------------------------------------------------------------------------|
| **`regressional_part/`** | ğŸ§  **Feature-level regression**: Wav2Vec2 embeddings â†’ statistical pooling â†’ MLP |
| **`transformer_part/`**  | ğŸ”¬ **Sequence-level regression**: Full token embeddings â†’ Transformer encoder  |

Each of these modules operates **independently** and includes its **own `train_final.py` and `main.ipynb`**, making it easy to evaluate or extend them in isolation.

---

### ğŸ“¦ Modular Folder Structure

```bash
GrammarScoring/
â”œâ”€â”€ dataset/                                # Audio + CSV data
â”‚   â”œâ”€â”€ audios_train/
â”‚   â”œâ”€â”€ audios_test/
â”‚   â”œâ”€â”€ train.csv
â”‚   â””â”€â”€ test.csv
â”‚
â”œâ”€â”€ regressional_part/                      # MLP Regression with pooled Wav2Vec2 features
â”‚   â”œâ”€â”€ train_final.py                      # Training + evaluation pipeline
â”‚   â”œâ”€â”€ main.ipynb                          # Visualizations & analysis
â”‚   â”œâ”€â”€ README.MD                           # Readme for this module
â”‚   â”œâ”€â”€ final_regression_model_submit.pt    # Modelweights
â”‚   â””â”€â”€ test_predictions_submit.csv         # Final predictions
â”‚
â”œâ”€â”€ transformer_part/                       # Transformer-based Regression
â”‚   â”œâ”€â”€ train_final.py                      # Sequence-level regression pipeline
â”‚   â”œâ”€â”€ main.ipynb                          # Visualizations & analysis
â”‚   â”œâ”€â”€ README.MD                           # Readme for this module
â”‚   â”œâ”€â”€ final_regression_model_submit.pt    # Modelweights
â”‚   â””â”€â”€ test_predictions_submit.csv         # Final predictions
â”‚
â”œâ”€â”€ .gitignore                              # .gitignore fiel 
â”œâ”€â”€ download_dataset.py                     # Run this first to setup the dataset folder
â”œâ”€â”€ requirements.txt                        # Requirements needed for this project  
â””â”€â”€ README.md                               # Project-wide technical summary
```

---

### ğŸ” How We Framed the Solution

Instead of treating this as a classic classification task, we modeled this as a **continuous-valued regression problem**, which is more aligned with the **subjective nature of MOS**. By leveraging **Wav2Vec2** for high-level speech representations, we removed the need for manual audio preprocessing, phoneme recognition, or grammar rule parsing.

We designed our models as follows:

| Design Element        | Details                                                                 |
|-----------------------|-------------------------------------------------------------------------|
| **Pretrained Features** | `torchaudio.pipelines.WAV2VEC2_BASE`, fine-tuned only on regression head |
| **MLP Head**            | Efficient inference-time performance for fast scoring                  |
| **Transformer Head**    | Better captures token-wise grammatical structure from speech embeddings |
| **Loss Function**       | Mean Squared Error (MSE) + output rounding to nearest 0.5              |
| **Evaluation**          | Performed in `.ipynb` inside each module using ground truth CSVs       |

---
