{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb09c9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, torch, torchaudio, pandas as pd\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbebfef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Environment Setup\n",
    "# -----------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "bundle = torchaudio.pipelines.WAV2VEC2_BASE\n",
    "wav2vec_model = bundle.get_model().to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a795cba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Configurations\n",
    "# -----------------------------\n",
    "TRAIN_AUDIO_DIR = \"../dataset/audios_train\"\n",
    "TEST_AUDIO_DIR = \"../dataset/audios_test\"\n",
    "TRAIN_CSV_PATH = \"../dataset/train.csv\"\n",
    "TEST_CSV_PATH = \"../dataset/test.csv\"\n",
    "\n",
    "FINAL_MODEL_PATH = \"final_transformer_model.pt\"\n",
    "PREDICTIONS_CSV = \"test_predictions_transformers_submit.csv\"\n",
    "\n",
    "BEST_LEARNING_RATE = 5e-4\n",
    "BEST_BATCH_SIZE = 8\n",
    "BEST_EPOCHS = 50\n",
    "BEST_NUM_HEADS = 4\n",
    "BEST_NUM_LAYERS = 3\n",
    "BEST_HIDDEN_DIM = 256\n",
    "BEST_DROPOUT = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e9a3d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Dataset Class\n",
    "# -----------------------------\n",
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, csv_path, audio_dir, is_test=False):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.is_test = is_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        waveform, sr = torchaudio.load(os.path.join(self.audio_dir, row['filename']))\n",
    "        if sr != bundle.sample_rate:\n",
    "            waveform = torchaudio.transforms.Resample(sr, bundle.sample_rate)(waveform)\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)\n",
    "        waveform = waveform.to(device)\n",
    "\n",
    "        if self.is_test:\n",
    "            return waveform, row['filename']\n",
    "        else:\n",
    "            label = torch.tensor(float(row['label']), dtype=torch.float32).to(device)\n",
    "            return waveform, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8ee4cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Transformer-Based Regressor\n",
    "# -----------------------------\n",
    "class TransformerRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, num_layers, hidden_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=input_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=hidden_dim,\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.attn_pool = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.LayerNorm(input_dim),\n",
    "            nn.Linear(input_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # x: [B, T, D]\n",
    "        x = self.encoder(x)  # [B, T, D]\n",
    "        attn_weights = self.attn_pool(x)  # [B, T, 1]\n",
    "        pooled = (attn_weights * x).sum(dim=1)  # [B, D]\n",
    "        return self.regressor(pooled).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e05933e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Feature Extraction\n",
    "# -----------------------------\n",
    "def extract_sequence_features(waveform):\n",
    "    with torch.no_grad():\n",
    "        if waveform.dim() == 3:\n",
    "            waveform = waveform.squeeze(1)\n",
    "        features, _ = wav2vec_model.extract_features(waveform)\n",
    "        return features[-1]  # [B, T, D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f21c299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Collate Functions\n",
    "# -----------------------------\n",
    "def collate_fn_train(batch):\n",
    "    waveforms = [x[0] for x in batch]\n",
    "    labels = torch.stack([x[1] for x in batch])\n",
    "    features = [extract_sequence_features(wf.unsqueeze(0)).squeeze(0).cpu() for wf in waveforms]\n",
    "    features_padded = pad_sequence(features, batch_first=True)  # [B, T, D]\n",
    "    return features_padded.to(device), labels\n",
    "\n",
    "def collate_fn_test(batch):\n",
    "    waveforms = [x[0] for x in batch]\n",
    "    filenames = [x[1] for x in batch]\n",
    "    features = [extract_sequence_features(wf.unsqueeze(0)).squeeze(0).cpu() for wf in waveforms]\n",
    "    features_padded = pad_sequence(features, batch_first=True)\n",
    "    return features_padded.to(device), filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "631c4491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Training Function\n",
    "# -----------------------------\n",
    "def train_model():\n",
    "    train_dataset = AudioDataset(TRAIN_CSV_PATH, TRAIN_AUDIO_DIR, is_test=False)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BEST_BATCH_SIZE, shuffle=True, collate_fn=collate_fn_train)\n",
    "\n",
    "    # Get input_dim from dummy input\n",
    "    input_dim = extract_sequence_features(torch.randn(1, 16000).to(device)).shape[2]\n",
    "    model = TransformerRegressor(\n",
    "        input_dim=input_dim,\n",
    "        num_heads=BEST_NUM_HEADS,\n",
    "        num_layers=BEST_NUM_LAYERS,\n",
    "        hidden_dim=BEST_HIDDEN_DIM,\n",
    "        dropout=BEST_DROPOUT\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=BEST_LEARNING_RATE)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.9, patience=2, min_lr=1e-6)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    print(\"üöÄ Starting training...\")\n",
    "    for epoch in range(BEST_EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        for feats, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{BEST_EPOCHS} [Train]\"):\n",
    "            preds = model(feats)\n",
    "            loss = criterion(preds, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {avg_loss:.4f} | LR = {current_lr:.6f}\")\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "    torch.save(model.state_dict(), FINAL_MODEL_PATH)\n",
    "    print(f\"‚úîÔ∏è Final model saved to {FINAL_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# Evaluation Function\n",
    "# -----------------------------\n",
    "def evaluate_model():\n",
    "    if not os.path.exists(FINAL_MODEL_PATH):\n",
    "        print(f\"‚ùå Error: Model not found at {FINAL_MODEL_PATH}\")\n",
    "        return\n",
    "\n",
    "    test_dataset = AudioDataset(TEST_CSV_PATH, TEST_AUDIO_DIR, is_test=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BEST_BATCH_SIZE, shuffle=False, collate_fn=collate_fn_test)\n",
    "\n",
    "    input_dim = extract_sequence_features(torch.randn(1, 16000).to(device)).shape[2]\n",
    "    model = TransformerRegressor(\n",
    "        input_dim=input_dim,\n",
    "        num_heads=BEST_NUM_HEADS,\n",
    "        num_layers=BEST_NUM_LAYERS,\n",
    "        hidden_dim=BEST_HIDDEN_DIM,\n",
    "        dropout=BEST_DROPOUT\n",
    "    ).to(device)\n",
    "    model.load_state_dict(torch.load(FINAL_MODEL_PATH))\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_filenames = []\n",
    "\n",
    "    print(\"üîç Evaluating model...\")\n",
    "    with torch.no_grad():\n",
    "        for feats, filenames in tqdm(test_loader, desc=\"Evaluation\"):\n",
    "            preds = model(feats)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_filenames.extend(filenames)\n",
    "\n",
    "    output_df = pd.DataFrame({\n",
    "        'filename': all_filenames,\n",
    "        'label': all_preds\n",
    "    })\n",
    "    output_df.iloc[:, 1] = output_df.iloc[:, 1].apply(lambda x: round(x * 2) / 2)\n",
    "    output_df.to_csv(PREDICTIONS_CSV, index=False)\n",
    "    print(f\"‚úîÔ∏è Predictions saved to {PREDICTIONS_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "32353d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 [Train]:   7%|‚ñã         | 4/56 [00:38<08:15,  9.53s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Entry Point\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# -----------------------------\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     evaluate_model()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     30\u001b[39m     loss.backward()\n\u001b[32m     31\u001b[39m     optimizer.step()\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     total_loss += \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m avg_loss = total_loss / \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[32m     35\u001b[39m current_lr = optimizer.param_groups[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Entry Point\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()\n",
    "    evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fdc7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = pd.read_csv(PREDICTIONS_CSV)\n",
    "# Plot histogram of predicted labels\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(output_df['label'], bins=[1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0, 5.5], edgecolor='black', rwidth=0.9)\n",
    "plt.xticks([1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0])\n",
    "plt.xlabel(\"Predicted MOS Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Distribution of Predicted Grammar Scores\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1990bccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by filename (or any order) for trendline visualization\n",
    "df_sorted = output_df.sort_values('filename')\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(df_sorted['label'].values, marker='o', linestyle='-', color='teal')\n",
    "plt.xlabel(\"Test Sample Index (sorted by filename)\")\n",
    "plt.ylabel(\"Predicted MOS Score\")\n",
    "plt.title(\"Trend of Predicted Grammar Scores\")\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a statistical summary\n",
    "plt.figure(figsize=(4, 6))\n",
    "plt.boxplot(output_df['label'], vert=True, patch_artist=True, boxprops=dict(facecolor='lightblue'))\n",
    "plt.ylabel(\"Predicted MOS Score\")\n",
    "plt.title(\"Boxplot of Predicted Grammar Scores\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.6)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
